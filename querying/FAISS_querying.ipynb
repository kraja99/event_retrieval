{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dccc17d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import faiss\n",
    "import nltk\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d323ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/news_narrative_splits/split1/news_narratives_train.pkl\", \"rb\") as outfile:\n",
    "    train_dict = pickle.load(outfile)\n",
    "with open(\"../data/news_narrative_splits/split1/news_narratives_test.pkl\", \"rb\") as outfile:\n",
    "    test_dict = pickle.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d81d7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "563065it [00:00, 1328311.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74589\n",
      "Within days of his inauguration , President Barack Obama signed executive orders to close the military prison at Guantanamo Bay within one year and to end torture in interrogation . He missed the Jan. 22 deadline to close Guantanamo but reaffirmed this month that he intends to close the prison as soon as possible . Obama has maintained other elements of the previous administration 's methods to capture and hold terrorism suspects . He has kept the military commission system to try certain terrorism suspects after strengthening evidentiary rules on behalf of defendants . He also preserved the authority to capture terrorism suspects in foreign countries , a practice known as extraordinary rendition . But he tightened the rules for where those captures can be made , limiting them to countries that do not have an effective rule of law . \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "news_filename = '../data/event_knowledge_2.0/news_narratives.txt'\n",
    "doc_to_text = {}\n",
    "last_doc_id = ''\n",
    "with open(news_filename) as news_narratives:\n",
    "    for line in tqdm(news_narratives):\n",
    "        if line == '\\n':\n",
    "            continue\n",
    "        if line[:8] == '<doc_id>':\n",
    "            doc_to_text[line[9:].replace('\\n', '')] = ''\n",
    "            last_doc_id = line[9:].replace('\\n', '')\n",
    "        if line[:6] == '<word>':\n",
    "            doc_to_text[last_doc_id] += line[7:].replace('\\n', ' ')\n",
    "print(len(doc_to_text))\n",
    "print(doc_to_text['WPB_ENG_20100127.0025.1:6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c807d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74589/74589 [00:02<00:00, 27326.43it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "corpus = [] # only has train items\n",
    "corpusidx_to_doc = {}\n",
    "idx = 0\n",
    "for k,v in tqdm(doc_to_text.items()):\n",
    "    if k not in train_dict.keys():\n",
    "        continue\n",
    "    sentences = sent_detector.tokenize(v)\n",
    "    for s in sentences:\n",
    "        corpus.append(s)\n",
    "        corpusidx_to_doc[idx] = k\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8535f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ef801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to generate embeddings\n",
    "# corpus_embeddings = model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n",
    "# with open(\"../data/news_narrative_splits/split1/news_narratives_train_emb.pt\", \"wb\") as outfile:\n",
    "#     torch.save(corpus_embeddings, outfile)\n",
    "\n",
    "corpus_embeddings = torch.load('../data/news_narrative_splits/split1/news_narratives_train_emb.pt', map_location=torch.device('cpu')).numpy()\n",
    "print(corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f3231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, model):\n",
    "    return np.expand_dims(model.encode(query, convert_to_numpy=True), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e37688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index using cosine similarity as distance metric\n",
    "def construct_index(corpus_embeddings):\n",
    "    index_f = faiss.index_factory(corpus_embeddings.shape[1], 'Flat', faiss.METRIC_INNER_PRODUCT)\n",
    "    res = faiss.StandardGpuResources()\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, index_f)\n",
    "    faiss.normalize_L2(corpus_embeddings)\n",
    "    index.add(corpus_embeddings)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8189555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = construct_index(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1ed37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging function\n",
    "def print_nearest_sents(sent_idxs, corpus):\n",
    "    sents = []\n",
    "    for idx in set(sent_idxs):\n",
    "        sents.append(corpus[idx])\n",
    "    return set(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f738fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Querying\n",
    "# sent = 'doctors removed her adrenal glands'\n",
    "# q = process_query(sent, model)\n",
    "# faiss.normalize_L2(q)\n",
    "# D, I = index.search(q, 5)\n",
    "# print(I)\n",
    "# print(D)\n",
    "# print_nearest_sents(I[0], corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d56d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events_before_after(query, k, index, corpus, doc_to_text, corpusidx_to_doc, model, anchor=None):\n",
    "    \n",
    "    events_before = []\n",
    "    events_after = []\n",
    "    \n",
    "    docs_found = []\n",
    "    related_sentences = []\n",
    "    \n",
    "    q = process_query(query, model)\n",
    "    faiss.normalize_L2(q)\n",
    "    D, I = index.search(q, k)\n",
    "    sent_idxs = I[0]\n",
    "    for idx in sent_idxs:\n",
    "        if anchor and anchor not in corpus[idx].split(\" \"): # Filtering for retrieved sentences w/o anchor\n",
    "            continue\n",
    "        related_sentences.append(corpus[idx])\n",
    "        doc = doc_to_text[corpusidx_to_doc[idx]]\n",
    "        docs_found.append(corpusidx_to_doc[idx])\n",
    "        anchor_idx = corpus_idx_to_idx_in_doc(idx, corpusidx_to_doc)\n",
    "        sentences = sent_detector.tokenize(doc)\n",
    "        for i, s in enumerate(sentences):\n",
    "            if len(s.split(\" \")) < 2: # Single word sentences aren't useful\n",
    "                continue\n",
    "            if i < anchor_idx - 3 or i > anchor_idx + 3: # only add sentences in +/- 3 sentence window\n",
    "                continue\n",
    "            if i < anchor_idx:\n",
    "                events_before.append(s)\n",
    "            elif i > anchor_idx:\n",
    "                events_after.append(s)\n",
    "# Debug output\n",
    "#     print(\"Query:\", query)\n",
    "#     print(\"Queried top\", k, \"sentences; Found\", len(set(docs_found)) , \"unique documents\")\n",
    "#     print(\"Related Sentences:\", len(related_sentences))\n",
    "#     for s in related_sentences:\n",
    "#         print(s)\n",
    "    return set(events_before), set(events_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2459022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_idx_to_idx_in_doc(corpus_idx, corpusidx_to_doc):\n",
    "    docid = corpusidx_to_doc[corpus_idx]\n",
    "    i = corpus_idx - 1\n",
    "    while i >= 0:\n",
    "        if corpusidx_to_doc[i] != docid:\n",
    "            return corpus_idx - i - 1\n",
    "        i -= 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9923b336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "def load_seeds(seed_path):\n",
    "    doc_sent_map = {}\n",
    "    with open(seed_path) as seeds:\n",
    "        for line in seeds:\n",
    "            text = line.split(\" - \")\n",
    "            doc_sent_map[text[0]] = text[1].strip()\n",
    "    return doc_sent_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23453e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsm = load_seeds('../data/news_narrative_splits/split1/seeds/bomb_seeds_test.txt')\n",
    "print(len(dsm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09aef4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if proposed event is similar to events in corpus_events (above threshold)\n",
    "def report_top_sim(proposed_event, corpus_events, model, threshold):\n",
    "    pe_embedding = model.encode(proposed_event, convert_to_tensor=True)\n",
    "    corpus_events_embeddings = model.encode(corpus_events, convert_to_tensor=True)\n",
    "    sims = []\n",
    "    for embedding in corpus_events_embeddings:\n",
    "        sim = util.cos_sim(pe_embedding, embedding)\n",
    "        sims.append(sim.detach()[0][0].cpu().numpy())\n",
    "    for i, s in enumerate(sims):\n",
    "        if s > threshold:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d556d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if proposed event happened before or after by seeing if the top-5 most similar sentences were from the before or after sets\n",
    "def before_or_after(proposed_event, events_before, events_after, model):\n",
    "    pe_embedding = model.encode(proposed_event, convert_to_tensor=True)\n",
    "    all_events = events_before + events_after\n",
    "    events_embeddings = model.encode(all_events, convert_to_tensor=True)\n",
    "    sims = []\n",
    "    for embedding in events_embeddings:\n",
    "        sim = util.cos_sim(pe_embedding, embedding)\n",
    "        sims.append(sim.detach()[0][0].cpu().numpy())\n",
    "    sims = np.array(sims)\n",
    "    print(np.argmax(sims))\n",
    "    max_sim_idx = np.argsort(-sims)[:5]\n",
    "    print(max_sim_idx)\n",
    "    count_before = 0\n",
    "    for idx in max_sim_idx:\n",
    "        if idx < len(events_before):\n",
    "            count_before += 1\n",
    "    return count_before >= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2837f1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "# adjust comments for was_before/report_top_sim depending on evaluation style\n",
    "\n",
    "sents_before = 0\n",
    "sents_before_correct = 0\n",
    "sents_after = 0\n",
    "sents_after_correct = 0\n",
    "for doc_id, anchor_sent in dsm.items():\n",
    "#     print(\"Anchor Sent:\", anchor_sent)\n",
    "    events_before, events_after = get_events_before_after(anchor_sent, 100, index, corpus, doc_to_text, corpusidx_to_doc, model)\n",
    "#     print(\"Events Before:\", len(events_before))\n",
    "#     for e in events_before:\n",
    "#         print(e)\n",
    "\n",
    "#     print(\"---\"*10)\n",
    "#     print(\"Events After:\", len(events_after))\n",
    "#     for e in events_after:\n",
    "#         print(e)\n",
    "#     print(\"---\"*10)\n",
    "    anchor_idx = -1\n",
    "    for i, s in enumerate(test_dict[doc_id]):\n",
    "        if anchor_sent == s:\n",
    "            anchor_idx = i\n",
    "    for i, s in enumerate(test_dict[doc_id]):\n",
    "        was_before = before_or_after(s, list(events_before), list(events_after), model) #\n",
    "        if i < anchor_idx:\n",
    "            sents_before += 1\n",
    "#             if report_top_sim(s, list(events_before), model, 0.6):\n",
    "#                 sents_before_correct += 1\n",
    "            if was_before: #\n",
    "                sents_before_correct += 1 #\n",
    "        elif i > anchor_idx:\n",
    "            sents_after += 1\n",
    "#             if report_top_sim(s, list(events_after), model, 0.6):\n",
    "#                 sents_after_correct += 1\n",
    "            if not was_before: #\n",
    "                sents_after_correct += 1 #\n",
    "print(\"From\", len(dsm), \"Seeds\")\n",
    "print(\"\\t\", sents_before_correct, \"/\", sents_before, \"=\", sents_before_correct/sents_before, \"On sentences BEFORE anchor\")\n",
    "print(\"\\t\", sents_after_correct, \"/\", sents_after, \"=\", sents_after_correct/sents_after, \"On sentences AFTER anchor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e73c0e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: smoke bomb burned several people\n",
      "Queried top 100 sentences; Found 94 unique documents\n",
      "Related Sentences: 100\n",
      "------------------------------\n",
      "Events Before: 164\n",
      "------------------------------\n",
      "Events After: 168\n"
     ]
    }
   ],
   "source": [
    "# Full Query Output\n",
    "query = 'smoke bomb burned several people'\n",
    "before, after = get_events_before_after(query, 100, index, corpus, doc_to_text, corpusidx_to_doc, model)\n",
    "# set of events disregards duplicate sentences (may come up when multiple sentences in top query results link back to same doc)\n",
    "print(\"---\"*10)\n",
    "print(\"Events Before:\", len(before))\n",
    "# for e in before:\n",
    "#     print(e)\n",
    "\n",
    "print(\"---\"*10)\n",
    "print(\"Events After:\", len(after))\n",
    "# for e in after:\n",
    "#     print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
